{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce25d8da",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdc8bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from openpyxl import Workbook, load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09d205",
   "metadata": {},
   "source": [
    "DCLRATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84e56f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"GlobVar.json\", \"r\") as file:\n",
    "#     gv = json.load(file)\n",
    "\n",
    "# mod_id = gv['mod_id']\n",
    "# MOD_ID = mod_id\n",
    "BATCH_ID = 5\n",
    "MOD_ID = 2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "TRANS_WEIGHT = 1.5\n",
    "ROTATION_WEIGHT = 1.0\n",
    "ANGULAR_WEIGHT = 0.1\n",
    "PATIENCE = 3\n",
    "IMG_SIZE = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a49f82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.expanduser(\"~/SKRIPSI/SCRIPTS\")\n",
    "DATASET_DIR = os.path.join(BASE_DIR, f\"dataset/batch{BATCH_ID}\")\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_DIR, f\"model/S-ConvNeXt6DP{BATCH_ID}.{MOD_ID}.pth\")\n",
    "BEST_MODEL_PATH = os.path.join(BASE_DIR, f\"model/BEST-S-ConvNeXt6DP{BATCH_ID}.{MOD_ID}.pth\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa324e1a",
   "metadata": {},
   "source": [
    "DATASETCLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "226657cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = torch.tensor(self.annotations.iloc[idx, 1:].astype(np.float32).values)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e732576",
   "metadata": {},
   "source": [
    "Conversions loss functions rmse yadaydadaydada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a11423d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_error(R_pred, R_gt):\n",
    "    \"\"\"Compute angular error in degrees between rotation matrices.\"\"\"\n",
    "    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = torch.diagonal(R_diff, dim1=1, dim2=2).sum(dim=1)\n",
    "    eps = 1e-6\n",
    "    angle_rad = torch.acos(torch.clamp((trace - 1) / 2, min=-1 + eps, max=1 - eps))\n",
    "    return torch.rad2deg(angle_rad)\n",
    "\n",
    "\n",
    "def geodesic_loss(R_pred, R_gt):\n",
    "    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = torch.diagonal(R_diff, dim1=1, dim2=2).sum(dim=1)\n",
    "    eps = 1e-6\n",
    "    angle = torch.acos(torch.clamp((trace - 1) / 2, -1 + eps, 1 - eps))\n",
    "    return angle.mean()\n",
    "\n",
    "def compute_rotation_matrix_from_ortho6d(poses_6d):\n",
    "    \"\"\"Convert 6D rotation representation to 3x3 rotation matrices.\"\"\"\n",
    "    x_raw = poses_6d[:, 0:3]\n",
    "    y_raw = poses_6d[:, 3:6]\n",
    "\n",
    "    x = F.normalize(x_raw, dim=1)\n",
    "    z = F.normalize(torch.cross(x, y_raw, dim=1), dim=1)\n",
    "    y = torch.cross(z, x, dim=1)\n",
    "\n",
    "    rot = torch.stack((x, y, z), dim=-1)  # Shape: [B, 3, 3]\n",
    "    return rot\n",
    "\n",
    "def combined_loss(output, target, trans_w=1.0, rot_w=1.0, ang_w=0.1):\n",
    "    pred_trans = output[:, :3]\n",
    "    gt_trans = target[:, :3]\n",
    "    pred_rot_6d = output[:, 3:9]\n",
    "    gt_rot_6d = target[:, 3:9]\n",
    "\n",
    "    pred_rot = compute_rotation_matrix_from_ortho6d(pred_rot_6d)\n",
    "    gt_rot = compute_rotation_matrix_from_ortho6d(gt_rot_6d)\n",
    "\n",
    "    loss_trans = F.mse_loss(pred_trans, gt_trans)\n",
    "    loss_rot = geodesic_loss(pred_rot, gt_rot)\n",
    "    return trans_w * loss_trans + rot_w * loss_rot\n",
    "\n",
    "\n",
    "def rotation_error_deg_from_6d(pred_6d, gt_6d):\n",
    "    # Ensure same dtype (float32)\n",
    "    pred_6d = pred_6d.float()\n",
    "    gt_6d = gt_6d.float()\n",
    "\n",
    "    R_pred = compute_rotation_matrix_from_ortho6d(pred_6d)\n",
    "    R_gt = compute_rotation_matrix_from_ortho6d(gt_6d)\n",
    "\n",
    "    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = R_diff[:, 0, 0] + R_diff[:, 1, 1] + R_diff[:, 2, 2]\n",
    "    cos_theta = (trace - 1) / 2\n",
    "    cos_theta = torch.clamp(cos_theta, -1.0, 1.0)\n",
    "\n",
    "    theta = torch.acos(cos_theta)\n",
    "    return torch.rad2deg(theta.mean())\n",
    "\n",
    "\n",
    "def compute_errors(outputs, labels):\n",
    "    outputs = outputs.float()\n",
    "    labels = labels.float()\n",
    "\n",
    "    pred_trans = outputs[:, :3]\n",
    "    pred_rot_6d = outputs[:, 3:9]\n",
    "\n",
    "    gt_trans = labels[:, :3]\n",
    "    gt_rot_6d = labels[:, 3:9]\n",
    "\n",
    "    trans_rmse = torch.sqrt(F.mse_loss(pred_trans, gt_trans))\n",
    "    rot_rmse = rotation_error_deg_from_6d(pred_rot_6d, gt_rot_6d)\n",
    "\n",
    "    return trans_rmse.item(), rot_rmse.item()\n",
    "\n",
    "\n",
    "def calculate_translation_rmse(preds, gts):\n",
    "    trans_rmse = np.sqrt(np.mean(np.sum((preds[:, :3] - gts[:, :3])**2, axis=1)))\n",
    "    return trans_rmse * 100  # Convert m → cm\n",
    "\n",
    "def translation_accuracy_percentage(rmse_cm, range_cm):\n",
    "    return max(0.0, 100.0 * (1 - rmse_cm / range_cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5faaf47",
   "metadata": {},
   "source": [
    "Transform & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe40d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "            std=[0.229, 0.224, 0.225]    # ImageNet std\n",
    "        )\n",
    "    ])\n",
    "\n",
    "def get_dataloader(split):\n",
    "    base_dir = os.path.join(DATASET_DIR, split)\n",
    "    csv_path = os.path.join(base_dir, \"labels.csv\")\n",
    "    images_dir = os.path.join(base_dir, \"images\")  # Updated to point to the actual image folder\n",
    "    dataset = PoseDataset(csv_path, images_dir, transform=get_transform())\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=(split == \"train\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda8eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_stats(loader):\n",
    "    translations = []\n",
    "    for _, labels in loader:\n",
    "        translations.append(labels[:, :3])\n",
    "    trans = torch.cat(translations, dim=0)\n",
    "    return {\n",
    "        'min': trans.min(dim=0).values,\n",
    "        'max': trans.max(dim=0).values,\n",
    "        'mean': trans.mean(dim=0),\n",
    "        'std': trans.std(dim=0)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7268fc10",
   "metadata": {},
   "source": [
    "Loading..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cc578b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(\"train\")\n",
    "val_loader = get_dataloader(\"val\")\n",
    "test_loader = get_dataloader(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc45f33",
   "metadata": {},
   "source": [
    "Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b1d4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXt6DP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNeXt6DP, self).__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            \"convnextv2_nano.fcmae_ft_in22k_in1k\",\n",
    "            pretrained=True,\n",
    "            features_only=False\n",
    "        )\n",
    "        self.backbone.head = nn.Identity()  # Remove classifier\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(640, 512),  # ✅ fix here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 9),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(128, 9)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.forward_features(x)  # Extract features\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590d1aa",
   "metadata": {},
   "source": [
    "The setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f77ec678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeXt6DP().to(DEVICE)  # Use the updated model class\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9eebe",
   "metadata": {},
   "source": [
    "Train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "404557a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(validate=True, resume_from_checkpoint=False):\n",
    "    writer = SummaryWriter(log_dir=os.path.join(BASE_DIR, f\"runs/S-ConvNeXt6DP_batch{BATCH_ID}.{MOD_ID}\"))\n",
    "    scaler = GradScaler()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    start_epoch = 0\n",
    "    now = []\n",
    "    time_per_epoch = []\n",
    "    # Resume from checkpoint if specified\n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        epochs_no_improve = checkpoint.get('epochs_no_improve', 0)\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        print(f\"✅ Resumed from checkpoint at epoch {start_epoch}\")\n",
    "        now = [0.0] * (start_epoch + 1)\n",
    "\n",
    "    if len(now) <= start_epoch:\n",
    "        now.append(time.time())\n",
    "    else:\n",
    "        now[start_epoch] = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "        print(\"\\n\")\n",
    "        print(f\"📦 EPOCH : {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\", leave=False)\n",
    "\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "                loss = combined_loss(outputs, labels, TRANS_WEIGHT, ROTATION_WEIGHT, ANGULAR_WEIGHT)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            torch.cuda.synchronize()  # ✅ Sync after each step\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        now.append(time.time())\n",
    "        time_per_epoch.append(int(now[epoch + 1] - now[epoch]))\n",
    "\n",
    "        print(f\"✅ Epoch {epoch + 1} Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"⏱️ Time per epoch {epoch + 1}: {time_per_epoch[epoch]}s\")\n",
    "\n",
    "        if epoch != 0 and epoch % 5 == 0:\n",
    "            parameters_to_prune = [\n",
    "                (module, 'weight') for module in model.modules()\n",
    "                if isinstance(module, (nn.Linear, nn.Conv2d)) and hasattr(module, 'weight')\n",
    "            ]\n",
    "            if parameters_to_prune:\n",
    "                prune.global_unstructured(\n",
    "                    parameters_to_prune,\n",
    "                    pruning_method=prune.L1Unstructured,\n",
    "                    amount=0.1\n",
    "                )\n",
    "                print(f\"⚠️ Pruning applied at epoch {epoch}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping pruning: No eligible parameters found at epoch {epoch}\")\n",
    "\n",
    "        if validate:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            total_trans_rmse, total_rot_rmse = 0.0, 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images = images.to(DEVICE)\n",
    "                    labels = labels.to(DEVICE)\n",
    "                    with autocast(device_type=\"cuda\"):\n",
    "                        outputs = model(images)\n",
    "                        loss = combined_loss(outputs, labels, TRANS_WEIGHT, ROTATION_WEIGHT, ANGULAR_WEIGHT)\n",
    "                    torch.cuda.synchronize()  # ✅ Sync for accurate timing\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    trans_rmse, rot_rmse = compute_errors(outputs, labels)\n",
    "                    total_trans_rmse += trans_rmse\n",
    "                    total_rot_rmse += rot_rmse\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            avg_trans_rmse = total_trans_rmse / len(val_loader)\n",
    "            avg_rot_rmse = total_rot_rmse / len(val_loader)\n",
    "\n",
    "            print(f\"📉 Validation Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"📐 RMSE - Translation: {avg_trans_rmse:.4f}, Rotation: {avg_rot_rmse:.4f}\")\n",
    "\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            if epoch != 0 and avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scaler_state_dict': scaler.state_dict(),\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'epochs_no_improve': epochs_no_improve,\n",
    "                    'epoch': epoch + 1\n",
    "                }, BEST_MODEL_PATH)\n",
    "                print(f\"Model saved to: {BEST_MODEL_PATH}\")\n",
    "                print(\"💾 Best model saved.\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f\"📉 No improvement ({epochs_no_improve}/{PATIENCE})\")\n",
    "                if epochs_no_improve >= PATIENCE:\n",
    "                    print(\"⏹️ Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'epochs_no_improve': epochs_no_improve,\n",
    "            'epoch': epoch + 1\n",
    "        }, MODEL_SAVE_PATH)\n",
    "        print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    writer.close()\n",
    "    return time_per_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f44a79",
   "metadata": {},
   "source": [
    "Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a1ecc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "📦 EPOCH : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 0/543 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 Avg Training Loss: 1.7465\n",
      "⏱️ Time per epoch 1: 269s\n",
      "📉 Validation Loss: 1.7518\n",
      "📐 RMSE - Translation: 0.1043, Rotation: 99.4308\n",
      "📉 No improvement (1/3)\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 Avg Training Loss: 1.4031\n",
      "⏱️ Time per epoch 2: 289s\n",
      "📉 Validation Loss: 1.1669\n",
      "📐 RMSE - Translation: 0.0860, Rotation: 66.2233\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 Avg Training Loss: 0.9437\n",
      "⏱️ Time per epoch 3: 291s\n",
      "📉 Validation Loss: 0.8077\n",
      "📐 RMSE - Translation: 0.0734, Rotation: 45.8136\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 Avg Training Loss: 0.6745\n",
      "⏱️ Time per epoch 4: 290s\n",
      "📉 Validation Loss: 0.6201\n",
      "📐 RMSE - Translation: 0.0626, Rotation: 35.1883\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 Avg Training Loss: 0.5110\n",
      "⏱️ Time per epoch 5: 285s\n",
      "📉 Validation Loss: 0.4769\n",
      "📐 RMSE - Translation: 0.0587, Rotation: 27.0265\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 Avg Training Loss: 0.3608\n",
      "⏱️ Time per epoch 6: 285s\n",
      "⚠️ Pruning applied at epoch 5\n",
      "📉 Validation Loss: 0.4039\n",
      "📐 RMSE - Translation: 0.0634, Rotation: 22.7990\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 Avg Training Loss: 0.2756\n",
      "⏱️ Time per epoch 7: 290s\n",
      "📉 Validation Loss: 0.3165\n",
      "📐 RMSE - Translation: 0.0503, Rotation: 17.9105\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 Avg Training Loss: 0.2241\n",
      "⏱️ Time per epoch 8: 290s\n",
      "📉 Validation Loss: 0.2790\n",
      "📐 RMSE - Translation: 0.0464, Rotation: 15.8035\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 Avg Training Loss: 0.1944\n",
      "⏱️ Time per epoch 9: 288s\n",
      "📉 Validation Loss: 0.2434\n",
      "📐 RMSE - Translation: 0.0453, Rotation: 13.7674\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 Avg Training Loss: 0.1749\n",
      "⏱️ Time per epoch 10: 290s\n",
      "📉 Validation Loss: 0.2268\n",
      "📐 RMSE - Translation: 0.0423, Rotation: 12.8492\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 11 Avg Training Loss: 0.1609\n",
      "⏱️ Time per epoch 11: 289s\n",
      "⚠️ Pruning applied at epoch 10\n",
      "📉 Validation Loss: 0.3103\n",
      "📐 RMSE - Translation: 0.0802, Rotation: 17.2279\n",
      "📉 No improvement (1/3)\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 12 Avg Training Loss: 0.1885\n",
      "⏱️ Time per epoch 12: 286s\n",
      "📉 Validation Loss: 0.1985\n",
      "📐 RMSE - Translation: 0.0427, Rotation: 11.2295\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 13 Avg Training Loss: 0.1242\n",
      "⏱️ Time per epoch 13: 288s\n",
      "📉 Validation Loss: 0.1686\n",
      "📐 RMSE - Translation: 0.0385, Rotation: 9.5489\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 14 Avg Training Loss: 0.1117\n",
      "⏱️ Time per epoch 14: 291s\n",
      "📉 Validation Loss: 0.1654\n",
      "📐 RMSE - Translation: 0.0377, Rotation: 9.3759\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 15 Avg Training Loss: 0.1083\n",
      "⏱️ Time per epoch 15: 289s\n",
      "📉 Validation Loss: 0.1524\n",
      "📐 RMSE - Translation: 0.0368, Rotation: 8.6244\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 16 Avg Training Loss: 0.0999\n",
      "⏱️ Time per epoch 16: 292s\n",
      "⚠️ Pruning applied at epoch 15\n",
      "📉 Validation Loss: 0.2505\n",
      "📐 RMSE - Translation: 0.1163, Rotation: 13.1782\n",
      "📉 No improvement (1/3)\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 17 Avg Training Loss: 0.1182\n",
      "⏱️ Time per epoch 17: 291s\n",
      "📉 Validation Loss: 0.1458\n",
      "📐 RMSE - Translation: 0.0444, Rotation: 8.2088\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 18 Avg Training Loss: 0.0914\n",
      "⏱️ Time per epoch 18: 291s\n",
      "📉 Validation Loss: 0.1305\n",
      "📐 RMSE - Translation: 0.0414, Rotation: 7.3521\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 19 Avg Training Loss: 0.0847\n",
      "⏱️ Time per epoch 19: 291s\n",
      "📉 Validation Loss: 0.1289\n",
      "📐 RMSE - Translation: 0.0393, Rotation: 7.2631\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 20 Avg Training Loss: 0.0793\n",
      "⏱️ Time per epoch 20: 290s\n",
      "📉 Validation Loss: 0.1253\n",
      "📐 RMSE - Translation: 0.0379, Rotation: 7.0809\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 21 Avg Training Loss: 0.0758\n",
      "⏱️ Time per epoch 21: 289s\n",
      "⚠️ Pruning applied at epoch 20\n",
      "📉 Validation Loss: 0.2815\n",
      "📐 RMSE - Translation: 0.1193, Rotation: 14.8979\n",
      "📉 No improvement (1/3)\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 22 Avg Training Loss: 0.1061\n",
      "⏱️ Time per epoch 22: 291s\n",
      "📉 Validation Loss: 0.1300\n",
      "📐 RMSE - Translation: 0.0431, Rotation: 7.3162\n",
      "📉 No improvement (2/3)\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 23 Avg Training Loss: 0.0739\n",
      "⏱️ Time per epoch 23: 291s\n",
      "📉 Validation Loss: 0.1130\n",
      "📐 RMSE - Translation: 0.0389, Rotation: 6.3572\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 24 Avg Training Loss: 0.0637\n",
      "⏱️ Time per epoch 24: 290s\n",
      "📉 Validation Loss: 0.1043\n",
      "📐 RMSE - Translation: 0.0366, Rotation: 5.8903\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 25 Avg Training Loss: 0.0605\n",
      "⏱️ Time per epoch 25: 289s\n",
      "📉 Validation Loss: 0.1028\n",
      "📐 RMSE - Translation: 0.0347, Rotation: 5.8119\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 26 Avg Training Loss: 0.0587\n",
      "⏱️ Time per epoch 26: 289s\n",
      "⚠️ Pruning applied at epoch 25\n",
      "📉 Validation Loss: 0.2937\n",
      "📐 RMSE - Translation: 0.0892, Rotation: 16.1416\n",
      "📉 No improvement (1/3)\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 27 Avg Training Loss: 0.0960\n",
      "⏱️ Time per epoch 27: 291s\n",
      "📉 Validation Loss: 0.1110\n",
      "📐 RMSE - Translation: 0.0376, Rotation: 6.2609\n",
      "📉 No improvement (2/3)\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 28 Avg Training Loss: 0.0632\n",
      "⏱️ Time per epoch 28: 290s\n",
      "📉 Validation Loss: 0.1018\n",
      "📐 RMSE - Translation: 0.0345, Rotation: 5.7633\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 29 Avg Training Loss: 0.0582\n",
      "⏱️ Time per epoch 29: 289s\n",
      "📉 Validation Loss: 0.1015\n",
      "📐 RMSE - Translation: 0.0342, Rotation: 5.7552\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n",
      "\n",
      "\n",
      "📦 EPOCH : 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 30 Avg Training Loss: 0.0532\n",
      "⏱️ Time per epoch 30: 291s\n",
      "📉 Validation Loss: 0.0879\n",
      "📐 RMSE - Translation: 0.0323, Rotation: 4.9756\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/BEST-S-ConvNeXt6DP5.2.pth\n",
      "💾 Best model saved.\n",
      "Model saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP5.2.pth\n"
     ]
    }
   ],
   "source": [
    "time_per_epoch = train(validate=True,resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec9dc5",
   "metadata": {},
   "source": [
    "Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc11afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gv['mod_id'] += 1\n",
    "# with open(\"GlobVar.json\", \"w\") as file:\n",
    "#     json.dump(gv, file, indent=4)\n",
    "# print(\"mod_id updated in GlobVar.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3142c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_MODEL_PATH = os.path.join(BASE_DIR, f\"model/CLEAN-S-ConvNeXt6DP{BATCH_ID}.{MOD_ID}.pth\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "        if hasattr(module, \"weight_orig\"):\n",
    "            prune.remove(module, \"weight\")\n",
    "torch.save(model.state_dict(), CLEAN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e26c11",
   "metadata": {},
   "source": [
    "Test func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd5658a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, mode='Test', use_amp=False):\n",
    "    model.eval()\n",
    "    inference_times = []\n",
    "    total_loss = 0.0\n",
    "    total_trans_rmse = 0.0\n",
    "    total_rot_rmse = 0.0\n",
    "    num_samples = 0\n",
    "    all_preds, all_gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=f\"Running {mode}\"):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast(device_type=\"cuda\"):\n",
    "                    outputs = model(images)\n",
    "                    loss = combined_loss(outputs, labels, TRANS_WEIGHT, ROTATION_WEIGHT, ANGULAR_WEIGHT)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = combined_loss(outputs, labels, TRANS_WEIGHT, ROTATION_WEIGHT, ANGULAR_WEIGHT)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            inference_time = (time.time() - start_time) * 1000 / batch_size  # ms per image\n",
    "            inference_times.append(inference_time)\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            trans_rmse, rot_rmse = compute_errors(outputs, labels)\n",
    "            total_trans_rmse += trans_rmse * batch_size\n",
    "            total_rot_rmse += rot_rmse * batch_size\n",
    "            num_samples += batch_size\n",
    "\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_gts.append(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / num_samples\n",
    "    avg_trans_rmse = total_trans_rmse / num_samples\n",
    "    avg_rot_rmse = total_rot_rmse / num_samples\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "\n",
    "    return avg_loss, avg_trans_rmse, avg_rot_rmse, np.concatenate(all_preds), np.concatenate(all_gts), avg_inference_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac87a4",
   "metadata": {},
   "source": [
    "Actually testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b170706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Test: 100%|██████████| 68/68 [00:24<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test Summary\n",
      "🔁 Average Loss       : 0.0875\n",
      "📐 Translation RMSE   : 0.0328\n",
      "📐 Rotation RMSE      : 4.9576\n",
      "⚡ Inference Time (ms) : 1.10 ms/image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(CLEAN_MODEL_PATH))\n",
    "model.to(DEVICE)\n",
    "\n",
    "test_avg_loss, test_avg_trans_rmse, test_avg_rot_rmse, test_preds, test_gts, test_avg_inference_time = test_model(\n",
    "    model, test_loader, mode='Test', use_amp=True\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Test Summary\")\n",
    "print(f\"🔁 Average Loss       : {test_avg_loss:.4f}\")\n",
    "print(f\"📐 Translation RMSE   : {test_avg_trans_rmse:.4f}\")\n",
    "print(f\"📐 Rotation RMSE      : {test_avg_rot_rmse:.4f}\")\n",
    "print(f\"⚡ Inference Time (ms) : {test_avg_inference_time:.2f} ms/image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c362dd",
   "metadata": {},
   "source": [
    "Val func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26e3119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model_path=None):\n",
    "    inference_times = []\n",
    "    if model_path:\n",
    "        if not os.path.exists(model_path):\n",
    "            raise ValueError(f\"Model path {model_path} does not exist.\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    total_loss, total_trans_rmse, total_rot_rmse = 0.0, 0.0, 0.0\n",
    "    num_samples = 0\n",
    "    all_preds, all_gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Running Validation\"):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "                loss = combined_loss(outputs, labels, TRANS_WEIGHT, ROTATION_WEIGHT, ANGULAR_WEIGHT)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            inference_time = (time.time() - start_time) * 1000 / batch_size  # ms per image\n",
    "            inference_times.append(inference_time)\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            trans_rmse, rot_rmse = compute_errors(outputs, labels)\n",
    "            total_rot_rmse += rot_rmse * batch_size\n",
    "            total_trans_rmse += trans_rmse * batch_size\n",
    "            num_samples += batch_size\n",
    "\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_gts.append(labels.cpu().numpy())\n",
    "\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    avg_loss = total_loss / num_samples\n",
    "    avg_trans_rmse = total_trans_rmse / num_samples\n",
    "    avg_rot_rmse = total_rot_rmse / num_samples\n",
    "\n",
    "    return avg_loss, avg_trans_rmse, avg_rot_rmse, np.concatenate(all_preds), np.concatenate(all_gts), avg_inference_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc60ceb",
   "metadata": {},
   "source": [
    "Actually validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bb4d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Validation: 100%|██████████| 68/68 [00:24<00:00,  2.80it/s]\n"
     ]
    }
   ],
   "source": [
    "val_avg_loss, val_avg_trans_rmse, val_avg_rot_rmse, val_preds, val_gts, val_avg_inference_time = validate_model(CLEAN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a75396b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e1754",
   "metadata": {},
   "source": [
    "Calculating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fb7bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_translation_rmse_cm = calculate_translation_rmse(test_preds, test_gts)\n",
    "val_translation_rmse_cm = calculate_translation_rmse(val_preds, val_gts)\n",
    "test_rot_accuracy = rotation_error_deg_from_6d(torch.tensor(test_preds[:, 3:]), torch.tensor(test_gts[:, 3:]))\n",
    "val_rot_accuracy = rotation_error_deg_from_6d(torch.tensor(val_preds[:, 3:]), torch.tensor(val_gts[:, 3:]))\n",
    "\n",
    "train_trans_stats = get_dataset_stats(train_loader)\n",
    "val_trans_stats = get_dataset_stats(val_loader)\n",
    "test_trans_stats = get_dataset_stats(test_loader)\n",
    "\n",
    "test_range_cm = (test_trans_stats['max'].mean() - test_trans_stats['min'].mean()) * 100\n",
    "val_range_cm = (val_trans_stats['max'].mean() - val_trans_stats['min'].mean()) * 100\n",
    "\n",
    "test_trans_accuracy_pct = translation_accuracy_percentage(test_translation_rmse_cm, test_range_cm).item()\n",
    "val_trans_accuracy_pct = translation_accuracy_percentage(val_translation_rmse_cm, val_range_cm).item()\n",
    "\n",
    "test_rot_accuracy_pct = 100*(1-(test_rot_accuracy.item()/360))\n",
    "val_rot_accuracy_pct = 100*(1-(val_rot_accuracy.item()/360))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98000c81",
   "metadata": {},
   "source": [
    "Write MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ecdbccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation report saved to: /home/moreno/SKRIPSI/SCRIPTS/model/S-ConvNeXt6DP_batch5.2.md\n"
     ]
    }
   ],
   "source": [
    "eval_path = os.path.join(BASE_DIR, f\"model/S-ConvNeXt6DP_batch{BATCH_ID}.{MOD_ID}.md\")\n",
    "eval_content = f\"\"\"# Evaluation Results - Batch {BATCH_ID} - Model {MOD_ID}\n",
    "\n",
    "## Training Configuration\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Epochs: {NUM_EPOCHS}\n",
    "- Learning Rate: {LEARNING_RATE}\n",
    "- Translation Weight : {TRANS_WEIGHT}\n",
    "- Rotation Weight : {ROTATION_WEIGHT}\n",
    "- Angular Weight : {ANGULAR_WEIGHT}\n",
    "- Patience : {PATIENCE}\n",
    "- Image Size: {IMG_SIZE}\n",
    "- Device: {DEVICE}\n",
    "- Optimizer : Adam\n",
    "\n",
    "## Model Architecture\n",
    "- Backbone: Using ConvNeXtV2 Nano @ {IMG_SIZE}\n",
    "- Head: Linear(620 -> 512 -> 9)\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### Test Set\n",
    "- Average Loss: {test_avg_loss:.4f}\n",
    "- Translation RMSE: {test_avg_trans_rmse:.4f}\n",
    "- Translation Accuracy: {test_translation_rmse_cm:.2f} cm\n",
    "- Translation Accuracy %: {test_trans_accuracy_pct:.2f}%\n",
    "- Rotation RMSE: {test_avg_rot_rmse :.4f}\n",
    "- Rotation Accuracy: {test_rot_accuracy:.2f}°\n",
    "- Rotation Accuracy % : {test_rot_accuracy_pct:.2f} %\n",
    "- Inference Speed: {test_avg_inference_time:.2f} ms/frame\n",
    "\n",
    "### Validation Set\n",
    "- Average Loss: {val_avg_loss:.4f}\n",
    "- Translation RMSE: {val_avg_trans_rmse :.4f}\n",
    "- Translation Accuracy: {val_translation_rmse_cm:.2f} cm\n",
    "- Translation Accuracy %: {val_trans_accuracy_pct:.2f}%\n",
    "- Rotation RMSE: {val_avg_rot_rmse :.4f}\n",
    "- Rotation Accuracy: {val_rot_accuracy:.2f}°\n",
    "- Rotation Accuracy % : {val_rot_accuracy_pct:.2f} %\n",
    "- Inference Speed: {val_avg_inference_time:.2f} ms/frame\n",
    "\n",
    "## Dataset Statistics\n",
    "### Training Set\n",
    "- Translation range: [{train_trans_stats['min'].mean():.2f}, {train_trans_stats['max'].mean():.2f}] m\n",
    "\n",
    "### Validation Set\n",
    "- Translation range: [{val_trans_stats['min'].mean():.2f}, {val_trans_stats['max'].mean():.2f}] m\n",
    "\n",
    "### Test Set\n",
    "- Translation range: [{test_trans_stats['min'].mean():.2f}, {test_trans_stats['max'].mean():.2f}] m\n",
    "\n",
    "## File Locations\n",
    "- Dataset Directory: {DATASET_DIR}\n",
    "- Model Save Path: {MODEL_SAVE_PATH}\n",
    "\"\"\"\n",
    "\n",
    "with open(eval_path, 'w') as f:\n",
    "    f.write(eval_content)\n",
    "print(f\"Evaluation report saved to: {eval_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04e82806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results appended to CSV: /home/moreno/SKRIPSI/SCRIPTS/model/eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "csv_path = os.path.join(BASE_DIR, \"model/eval_results.csv\")\n",
    "write_header = not os.path.exists(csv_path)\n",
    "\n",
    "csv_data = {\n",
    "    'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_id': BATCH_ID,\n",
    "    'model_id': MOD_ID,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'translation_weight':TRANS_WEIGHT,\n",
    "    'rotation_weight' : ROTATION_WEIGHT,\n",
    "    'angular_weight' : ANGULAR_WEIGHT,\n",
    "    'patience' : PATIENCE,\n",
    "    'test_loss': test_avg_loss,\n",
    "    'test_translation_rmse': test_avg_trans_rmse,\n",
    "    'test_translation_accuracy_pct': test_trans_accuracy_pct,\n",
    "    'test_rotation_rmse': test_avg_rot_rmse ,\n",
    "    'test_rotation_accuracy_pct':test_rot_accuracy_pct,\n",
    "    'test_inference_time_ms': test_avg_inference_time,\n",
    "    'validation_loss': val_avg_loss,\n",
    "    'validation_translation_rmse': val_avg_trans_rmse ,\n",
    "    'validation_translation_accuracy_pct': val_trans_accuracy_pct,\n",
    "    'validation_rotation_rmse': val_avg_rot_rmse ,\n",
    "    'validation_rotation_accuracy_pct':val_rot_accuracy_pct,\n",
    "    'validation_inference_time_ms': val_avg_inference_time,\n",
    "    'model_path': MODEL_SAVE_PATH,\n",
    "    'eval_path' : eval_path\n",
    "}\n",
    "\n",
    "with open(csv_path, 'a', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_data.keys())\n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(csv_data)\n",
    "print(f\"Results appended to CSV: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74271b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation logged to Excel: /home/moreno/SKRIPSI/SCRIPTS/model/EVAL.xlsx\n"
     ]
    }
   ],
   "source": [
    "def safe(val):\n",
    "    if isinstance(val, (np.floating, np.integer)):\n",
    "        return val.item()\n",
    "    return val\n",
    "\n",
    "def log_eval_to_xlsx(\n",
    "    variant=\"VANILA\",\n",
    "    head_arch=\"640 → 512 → 9\",\n",
    "    param_count=332288,\n",
    "    notes=\"\"\"sanity check on 51 because changed the RMSE calculations considerably also changed epoch to 30 to see if there's some more performance to be taken from, changed from 385 to 224\n",
    "            JUST LET IT RUN IT'S COURSE\n",
    "            \"\"\",\n",
    "    xlsx_path=os.path.join(BASE_DIR, \"model/EVAL.xlsx\")\n",
    "):\n",
    "    headers = [\n",
    "        \"ID\", \"IMG_SIZE\", \"VARIANT\", \"HEAD_ARCH\", \"PARAM_COUNT\", \"BATCH_SIZE\", \"EPC\", \"DTS_ID\", \"DTS_LEN\", \"LR_RT\",\n",
    "        \"TRAIN_TIME\", \"VRAM_USG\", \"VAL_LOSS\", \"TS_LOSS\",\n",
    "        \"VAL_TRANS_RSME\", \"TS_TRANS_RMSE\", \"VAL_ROT_RSME\", \"TS_ROT_RMSE\",\n",
    "        \"VAL_TRANS_ACC\", \"TS_TRANS_ACC\", \"VAL_ROT_ACC\", \"TS_ROT_ACC\",\n",
    "        \"VAL_INF_MS\", \"TS_INF_MS\", \"Notes\"\n",
    "    ]\n",
    "\n",
    "    model_id_str = f\"{BATCH_ID}{MOD_ID}\"\n",
    "    dataset_len = len(train_loader.dataset) + len(val_loader.dataset) + len(test_loader.dataset)\n",
    "\n",
    "    row = [\n",
    "        int(model_id_str), IMG_SIZE, variant, head_arch, param_count, BATCH_SIZE, NUM_EPOCHS,\n",
    "        BATCH_ID, dataset_len, str(LEARNING_RATE),  # Still keep LR as string\n",
    "        round(sum(time_per_epoch)/60,1), 2.7,  # TRAIN_TIME, VRAM_USG\n",
    "\n",
    "        round(val_avg_loss, 4), round(test_avg_loss, 4),\n",
    "        round(val_avg_trans_rmse , 4),\n",
    "        round(test_avg_trans_rmse, 4),\n",
    "        round(val_avg_rot_rmse , 4),\n",
    "        round(test_avg_rot_rmse , 4),\n",
    "\n",
    "        round(val_trans_accuracy_pct, 2), round(test_trans_accuracy_pct, 2),\n",
    "        round(val_rot_accuracy_pct, 2), round(test_rot_accuracy_pct, 2),\n",
    "\n",
    "        round(val_avg_inference_time, 2), round(test_avg_inference_time, 2),\n",
    "\n",
    "        notes\n",
    "    ]\n",
    "\n",
    "\n",
    "    row = [safe(x) for x in row]\n",
    "\n",
    "    # Load or create workbook\n",
    "    if os.path.exists(xlsx_path):\n",
    "        wb = load_workbook(xlsx_path)\n",
    "        ws = wb.active\n",
    "    else:\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        ws.title = \"ConvNeXt V2 Nano\"\n",
    "        ws.append(headers)\n",
    "\n",
    "    ws.append(row)\n",
    "    wb.save(xlsx_path)\n",
    "    print(f\"✅ Evaluation logged to Excel: {xlsx_path}\")\n",
    "\n",
    "\n",
    "log_eval_to_xlsx()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
