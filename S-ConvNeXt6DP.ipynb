{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce25d8da",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09d205",
   "metadata": {},
   "source": [
    "DCLRATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e56f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"GlobVar.json\", \"r\") as file:\n",
    "    gv = json.load(file)\n",
    "\n",
    "mod_id = gv['mod_id']\n",
    "\n",
    "BATCH_ID = 5\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "TRANS_WEIGHT = 1.5\n",
    "ROTATION_WEIGHT = 1.0\n",
    "ANGULAR_WEIGHT = 0.1\n",
    "PATIENCE = 3\n",
    "IMG_SIZE = 384\n",
    "BASE_DIR = os.path.expanduser(\"~/SKRIPSI/SCRIPTS\")\n",
    "DATASET_DIR = os.path.join(BASE_DIR, f\"dataset/batch{BATCH_ID}\")\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_DIR, f\"model/S-ConvNeXt6DP{BATCH_ID}.{mod_id}.pth\")\n",
    "BEST_MODEL_PATH = os.path.join(BASE_DIR, f\"model/BEST-S-ConvNeXt6DP{BATCH_ID}.{mod_id}.pth\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa324e1a",
   "metadata": {},
   "source": [
    "DATASETCLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226657cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = torch.tensor(self.annotations.iloc[idx, 1:].values, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e732576",
   "metadata": {},
   "source": [
    "Conversions loss functions rmse yadaydadaydada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11423d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_error(R_pred, R_gt):\n",
    "    \"\"\"Compute angular error in degrees between rotation matrices.\"\"\"\n",
    "    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = torch.diagonal(R_diff, dim1=1, dim2=2).sum(dim=1)\n",
    "    eps = 1e-6\n",
    "    angle_rad = torch.acos(torch.clamp((trace - 1) / 2, min=-1 + eps, max=1 - eps))\n",
    "    return torch.rad2deg(angle_rad)\n",
    "\n",
    "\n",
    "def geodesic_loss(R_pred, R_gt):\n",
    "    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = torch.diagonal(R_diff, dim1=1, dim2=2).sum(dim=1)\n",
    "    eps = 1e-6\n",
    "    angle = torch.acos(torch.clamp((trace - 1) / 2, -1 + eps, 1 - eps))\n",
    "    return angle.mean()\n",
    "\n",
    "def compute_rotation_matrix_from_ortho6d(poses_6d):\n",
    "    \"\"\"Convert 6D rotation representation to 3x3 rotation matrices.\"\"\"\n",
    "    x_raw = poses_6d[:, 0:3]\n",
    "    y_raw = poses_6d[:, 3:6]\n",
    "\n",
    "    x = F.normalize(x_raw, dim=1)\n",
    "    z = F.normalize(torch.cross(x, y_raw, dim=1), dim=1)\n",
    "    y = torch.cross(z, x, dim=1)\n",
    "\n",
    "    rot = torch.stack((x, y, z), dim=-1)  # Shape: [B, 3, 3]\n",
    "    return rot\n",
    "\n",
    "def combined_loss(output, target, trans_w=1.0, rot_w=1.0, ang_w=0.1):\n",
    "    pred_trans = output[:, :3]\n",
    "    gt_trans = target[:, :3]\n",
    "    pred_rot_6d = output[:, 3:9]\n",
    "    gt_rot_6d = target[:, 3:9]\n",
    "\n",
    "    pred_rot = compute_rotation_matrix_from_ortho6d(pred_rot_6d)\n",
    "    gt_rot = compute_rotation_matrix_from_ortho6d(gt_rot_6d)\n",
    "\n",
    "    loss_trans = F.mse_loss(pred_trans, gt_trans)\n",
    "    loss_rot = geodesic_loss(pred_rot, gt_rot)\n",
    "    return trans_w * loss_trans + rot_w * loss_rot\n",
    "\n",
    "\n",
    "def rotation_error_deg_from_6d(pred_6d, gt_6d):\n",
    "    R_pred = compute_rotation_matrix_from_ortho6d(pred_6d)\n",
    "    R_gt = compute_rotation_matrix_from_ortho6d(gt_6d)\n",
    "\n",
    "    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = R_diff[:, 0, 0] + R_diff[:, 1, 1] + R_diff[:, 2, 2]\n",
    "    cos_theta = (trace - 1) / 2\n",
    "    cos_theta = torch.clamp(cos_theta, -1.0 + 1e-6, 1.0 - 1e-6)\n",
    "    theta = torch.acos(cos_theta)\n",
    "    return torch.rad2deg(theta).mean()\n",
    "\n",
    "def compute_errors(outputs, labels):\n",
    "    pred_trans = outputs[:, :3]\n",
    "    gt_trans = labels[:, :3]\n",
    "\n",
    "    pred_rot_6d = outputs[:, 3:9]\n",
    "    gt_rot_6d = labels[:, 3:9]\n",
    "\n",
    "    trans_rmse = torch.sqrt(F.mse_loss(pred_trans, gt_trans))\n",
    "    rot_rmse = rotation_error_deg_from_6d(pred_rot_6d, gt_rot_6d)\n",
    "    return trans_rmse.item(), rot_rmse.item()\n",
    "\n",
    "def calculate_translation_rmse(preds, gts):\n",
    "    trans_rmse = np.sqrt(np.mean(np.sum((preds[:, :3] - gts[:, :3])**2, axis=1)))\n",
    "    return trans_rmse * 100  # Convert m ‚Üí cm\n",
    "\n",
    "def translation_accuracy_percentage(rmse_cm, range_cm):\n",
    "    return max(0.0, 100.0 * (1 - rmse_cm / range_cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5faaf47",
   "metadata": {},
   "source": [
    "Transform & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "            std=[0.229, 0.224, 0.225]    # ImageNet std\n",
    "        )\n",
    "    ])\n",
    "\n",
    "def get_dataloader(split):\n",
    "    csv_path = os.path.join(DATASET_DIR, f\"{split}.csv\")\n",
    "    images_dir = os.path.join(DATASET_DIR, split)\n",
    "    dataset = PoseDataset(csv_path, images_dir, transform=get_transform())\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=(split == \"train\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_stats(loader):\n",
    "    translations = []\n",
    "    for _, labels in loader:\n",
    "        translations.append(labels[:, :3])\n",
    "    trans = torch.cat(translations, dim=0)\n",
    "    return {\n",
    "        'min': trans.min(dim=0).values,\n",
    "        'max': trans.max(dim=0).values,\n",
    "        'mean': trans.mean(dim=0),\n",
    "        'std': trans.std(dim=0)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7268fc10",
   "metadata": {},
   "source": [
    "Loading..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc578b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(\"train\")\n",
    "val_loader = get_dataloader(\"val\")\n",
    "test_loader = get_dataloader(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc45f33",
   "metadata": {},
   "source": [
    "Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXt6DP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNeXt6DP, self).__init__()\n",
    "        self.backbone = timm.create_model(\"convnextv2_nano.fcmae_ft_in22k_in1k_384\", pretrained=True)\n",
    "        self.backbone.head = nn.Identity()  # Remove the classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 9)  # 3 translation + 6 rotation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590d1aa",
   "metadata": {},
   "source": [
    "The setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ec678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeXt6DP().to(DEVICE)  # Use the updated model class\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9eebe",
   "metadata": {},
   "source": [
    "Train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404557a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(validate=True, resume_from_checkpoint=False):\n",
    "    scaler = GradScaler()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    now = [time.time()]\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Resume from checkpoint if specified\n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "        epochs_no_improve = checkpoint.get('epochs_no_improve', 0)\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        print(f\"‚úÖ Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "        print(\"\\n\")\n",
    "        print(f\"üì¶ EPOCH : {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\", leave=False)\n",
    "\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = combined_loss(outputs, labels, TRANS_WEIGHT, ROTATION_WEIGHT, ANGULAR_WEIGHT)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"‚úÖ Epoch {epoch + 1} Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "        now.append(time.time())\n",
    "        print(f\"‚è±Ô∏è Time per epoch {epoch + 1}: {int(now[epoch + 1] - now[epoch])}s\")\n",
    "\n",
    "        # Apply structured pruning periodically (skip epoch 0)\n",
    "        if epoch != 0 and epoch % 5 == 0:\n",
    "            parameters_to_prune = (\n",
    "                (module, 'weight') for module in model.modules()\n",
    "                if isinstance(module, (nn.Linear, nn.Conv2d))\n",
    "            )\n",
    "            prune.global_unstructured(\n",
    "                parameters_to_prune,\n",
    "                pruning_method=prune.L1Unstructured,\n",
    "                amount=0.1\n",
    "            )\n",
    "            print(f\"‚ö†Ô∏è Pruning applied at epoch {epoch}\")\n",
    "\n",
    "        if validate:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            total_trans_rmse, total_rot_rmse = 0.0, 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images = images.to(DEVICE)\n",
    "                    labels = labels.to(DEVICE)\n",
    "                    outputs = model(images)\n",
    "\n",
    "                    loss = combined_loss(outputs, labels, TRANS_WEIGHT, ROTATION_WEIGHT, ANGULAR_WEIGHT)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    trans_rmse, rot_rmse = compute_errors(outputs, labels)\n",
    "                    total_trans_rmse += trans_rmse\n",
    "                    total_rot_rmse += rot_rmse\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            avg_trans_rmse = total_trans_rmse / len(val_loader)\n",
    "            avg_rot_rmse = total_rot_rmse / len(val_loader)\n",
    "\n",
    "            print(f\"üìâ Validation Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"üìê RMSE - Translation: {avg_trans_rmse:.4f}, Rotation: {avg_rot_rmse:.4f}\")\n",
    "\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            # Save best model if validation improves\n",
    "            if epoch != 0 and avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scaler_state_dict': scaler.state_dict(),\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'epochs_no_improve': epochs_no_improve,\n",
    "                    'epoch': epoch + 1\n",
    "                }, BEST_MODEL_PATH)\n",
    "                print(f\"Model saved to: {BEST_MODEL_PATH}\")\n",
    "                print(\"üíæ Best model saved.\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f\"üìâ No improvement ({epochs_no_improve}/{PATIENCE})\")\n",
    "\n",
    "                if epochs_no_improve >= PATIENCE:\n",
    "                    print(\"‚èπÔ∏è Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "        # Always save last checkpoint\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'epochs_no_improve': epochs_no_improve,\n",
    "            'epoch': epoch + 1\n",
    "        }, MODEL_SAVE_PATH)\n",
    "        print(f\"Model saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f44a79",
   "metadata": {},
   "source": [
    "Actually training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ecc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(validate=True,resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec9dc5",
   "metadata": {},
   "source": [
    "Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv['mod_id'] += 1\n",
    "with open(\"GlobVar.json\", \"w\") as file:\n",
    "    json.dump(gv, file, indent=4)\n",
    "print(\"mod_id updated in GlobVar.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e26c11",
   "metadata": {},
   "source": [
    "Test func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5658a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, mode='Test', use_amp=False):\n",
    "    inference_times = []\n",
    "    model.eval()\n",
    "    total_loss, total_trans_rmse, total_rot_rmse = 0, 0, 0\n",
    "    all_preds, all_gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=f\"Running {mode}\"):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            outputs = model(images)\n",
    "            inference_time = (time.time() - start_time) * 1000 / images.size(0)  # ms per image\n",
    "            inference_times.append(inference_time)\n",
    "            \n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            # Optionally use mixed precision for testing\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = combined_loss(outputs, labels, TRANS_WEIGHT, ROTATION_WEIGHT, ANGULAR_WEIGHT)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = combined_loss(outputs, labels, TRANS_WEIGHT, ROTATION_WEIGHT, ANGULAR_WEIGHT)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            trans_rmse, rot_rmse = compute_errors(outputs, labels)\n",
    "            total_rot_rmse += rot_rmse\n",
    "            total_trans_rmse += trans_rmse\n",
    "\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_gts.append(labels.cpu().numpy())\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, total_trans_rmse, total_rot_rmse, np.concatenate(all_preds), np.concatenate(all_gts), avg_inference_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac87a4",
   "metadata": {},
   "source": [
    "Actually testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b170706",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "model.to(DEVICE)\n",
    "test_avg_loss, test_total_trans_rmse, test_total_rot_rmse, test_preds, test_gts, test_avg_inference_time = test_model(model, test_loader, mode='Test', use_amp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c362dd",
   "metadata": {},
   "source": [
    "Val func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model_path=None):\n",
    "    inference_times = []\n",
    "    if model_path:\n",
    "        if not os.path.exists(model_path):\n",
    "            raise ValueError(f\"Model path {model_path} does not exist.\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    total_loss, total_trans_rmse, total_rot_rmse = 0, 0, 0\n",
    "    all_preds, all_gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Running Validation\"):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            outputs = model(images)\n",
    "            inference_time = (time.time() - start_time) * 1000 / images.size(0)  # ms per image\n",
    "            inference_times.append(inference_time)\n",
    "\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            with torch.cuda.amp.autocast():  # Optional for mixed precision inference\n",
    "                outputs = model(images)\n",
    "                loss = combined_loss(outputs, labels, TRANS_WEIGHT, ROTATION_WEIGHT, ANGULAR_WEIGHT)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            trans_rmse, rot_rmse = compute_errors(outputs, labels)\n",
    "            total_rot_rmse += rot_rmse\n",
    "            total_trans_rmse += trans_rmse\n",
    "\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_gts.append(labels.cpu().numpy())\n",
    "\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss, total_trans_rmse, total_rot_rmse, np.concatenate(all_preds), np.concatenate(all_gts), avg_inference_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc60ceb",
   "metadata": {},
   "source": [
    "Actually validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_avg_loss, val_total_trans_rmse, val_total_rot_rmse, val_preds, val_gts, val_avg_inference_time = validate_model(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75396b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e1754",
   "metadata": {},
   "source": [
    "Calculating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_translation_rmse_cm = calculate_translation_rmse(test_preds, test_gts)\n",
    "val_translation_rmse_cm = calculate_translation_rmse(val_preds, val_gts)\n",
    "test_rot_accuracy = rotation_error_deg_from_6d(torch.tensor(test_preds[:, 3:]), torch.tensor(test_gts[:, 3:]))\n",
    "val_rot_accuracy = rotation_error_deg_from_6d(torch.tensor(val_preds[:, 3:]), torch.tensor(val_gts[:, 3:]))\n",
    "\n",
    "train_trans_stats = get_dataset_stats(train_loader)\n",
    "val_trans_stats = get_dataset_stats(val_loader)\n",
    "test_trans_stats = get_dataset_stats(test_loader)\n",
    "\n",
    "test_range_cm = (test_trans_stats['max'].mean() - test_trans_stats['min'].mean()) * 100\n",
    "val_range_cm = (val_trans_stats['max'].mean() - val_trans_stats['min'].mean()) * 100\n",
    "\n",
    "test_trans_accuracy_pct = translation_accuracy_percentage(test_translation_rmse_cm, test_range_cm)\n",
    "val_trans_accuracy_pct = translation_accuracy_percentage(val_translation_rmse_cm, val_range_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98000c81",
   "metadata": {},
   "source": [
    "Write MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecdbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = os.path.join(BASE_DIR, f\"model/ViT6DP_EVAL_batch{BATCH_ID}.{mod_id-1}.md\")\n",
    "eval_content = f\"\"\"# Evaluation Results - Batch {BATCH_ID} - Model {mod_id-1}\n",
    "\n",
    "## Training Configuration\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Epochs: {NUM_EPOCHS}\n",
    "- Learning Rate: {LEARNING_RATE}\n",
    "- Translation Weight : {TRANS_WEIGHT}\n",
    "- Rotation Weight : {ROTATION_WEIGHT}\n",
    "- Angular Weight : {ANGULAR_WEIGHT}\n",
    "- Patience : {PATIENCE}\n",
    "- Image Size: {IMG_SIZE}\n",
    "- Device: {DEVICE}\n",
    "- Optimizer : Adam\n",
    "\n",
    "## Model Architecture\n",
    "- Backbone: Using ConvNeXt\n",
    "- Head: Linear(768->512->9)\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### Test Set\n",
    "- Average Loss: {test_avg_loss:.4f}\n",
    "- Translation RMSE: {test_total_trans_rmse / len(test_loader):.4f}\n",
    "- Translation Accuracy: {test_translation_rmse_cm:.2f} cm\n",
    "- Translation Accuracy %: {test_trans_accuracy_pct:.2f}%\n",
    "- Rotation RMSE: {test_total_rot_rmse / len(test_loader):.4f}\n",
    "- Rotation Accuracy: {test_rot_accuracy:.2f}¬∞\n",
    "- Inference Speed: {test_avg_inference_time:.2f} ms/frame\n",
    "\n",
    "### Validation Set\n",
    "- Average Loss: {val_avg_loss:.4f}\n",
    "- Translation RMSE: {val_total_trans_rmse / len(val_loader):.4f}\n",
    "- Translation Accuracy: {val_translation_rmse_cm:.2f} cm\n",
    "- Translation Accuracy %: {val_trans_accuracy_pct:.2f}%\n",
    "- Rotation RMSE: {val_total_rot_rmse / len(val_loader):.4f}\n",
    "- Rotation Accuracy: {val_rot_accuracy:.2f}¬∞\n",
    "- Inference Speed: {val_avg_inference_time:.2f} ms/frame\n",
    "\n",
    "## Dataset Statistics\n",
    "### Training Set\n",
    "- Translation range: [{train_trans_stats['min'].mean():.2f}, {train_trans_stats['max'].mean():.2f}] m\n",
    "\n",
    "### Validation Set\n",
    "- Translation range: [{val_trans_stats['min'].mean():.2f}, {val_trans_stats['max'].mean():.2f}] m\n",
    "\n",
    "### Test Set\n",
    "- Translation range: [{test_trans_stats['min'].mean():.2f}, {test_trans_stats['max'].mean():.2f}] m\n",
    "\n",
    "## File Locations\n",
    "- Dataset Directory: {DATASET_DIR}\n",
    "- Model Save Path: {MODEL_SAVE_PATH}\n",
    "\"\"\"\n",
    "\n",
    "with open(eval_path, 'w') as f:\n",
    "    f.write(eval_content)\n",
    "print(f\"Evaluation report saved to: {eval_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e82806",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(BASE_DIR, \"model/eval_results.csv\")\n",
    "write_header = not os.path.exists(csv_path)\n",
    "\n",
    "csv_data = {\n",
    "    'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_id': BATCH_ID,\n",
    "    'model_id': mod_id-1,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'translation_weight':TRANS_WEIGHT,\n",
    "    'rotation_weight' : ROTATION_WEIGHT,\n",
    "    'angular_weight' : ANGULAR_WEIGHT,\n",
    "    'patience' : PATIENCE,\n",
    "    'test_loss': test_avg_loss,\n",
    "    'test_translation_rmse': test_total_trans_rmse / len(test_loader),\n",
    "    'test_translation_accuracy_pct': test_trans_accuracy_pct,\n",
    "    'test_rotation_rmse': test_total_rot_rmse / len(test_loader),\n",
    "    'test_inference_time_ms': test_avg_inference_time,\n",
    "    'validation_loss': val_avg_loss,\n",
    "    'validation_translation_rmse': val_total_trans_rmse / len(val_loader),\n",
    "    'validation_translation_accuracy_pct': val_trans_accuracy_pct,\n",
    "    'validation_rotation_rmse': val_total_rot_rmse / len(val_loader),\n",
    "    'validation_inference_time_ms': val_avg_inference_time,\n",
    "    'model_path': MODEL_SAVE_PATH,\n",
    "    'eval_path' : eval_path\n",
    "}\n",
    "\n",
    "with open(csv_path, 'a', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_data.keys())\n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(csv_data)\n",
    "print(f\"Results appended to CSV: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
