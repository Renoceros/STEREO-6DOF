{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIT6D.ipynb refactored for Minty (Linux Mint)\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99472d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load global variables\n",
    "with open(\"GlobVar.json\", \"r\") as file:\n",
    "    gv = json.load(file)\n",
    "\n",
    "mod_id = gv['mod_id']\n",
    "\n",
    "# Constants and environment setup\n",
    "BATCH_ID = 5\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "TRANS_WEIGHT = 1.5 \n",
    "ROTATION_WEIGHT = 1.0\n",
    "ANGULAR_WEIGHT = 0.1\n",
    "PATIENCE = 3\n",
    "IMG_SIZE = 224  # Required input size for ViT\n",
    "BASE_DIR = os.path.expanduser(\"~/SKRIPSI/SCRIPTS\")\n",
    "DATASET_DIR = os.path.join(BASE_DIR, f\"dataset/batch{BATCH_ID}\")\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_DIR, f\"model/ViT6DP_batch{BATCH_ID}.{mod_id}.pth\")\n",
    "\n",
    "# Use CUDA if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0be15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_csv, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = pd.read_csv(label_csv)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['image_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = torch.tensor(row[1:].values.astype('float32'))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b917d6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def sixd_to_rotation_matrix(sixd):\n",
    "    \"\"\"Convert 6D rotation representation to 3x3 rotation matrix.\"\"\"\n",
    "    a1, a2 = sixd[:, :3], sixd[:, 3:]\n",
    "    b1 = a1 / torch.norm(a1, dim=1, keepdim=True)\n",
    "    b2 = a2 - torch.sum(b1 * a2, dim=1, keepdim=True) * b1\n",
    "    b2 = b2 / torch.norm(b2, dim=1, keepdim=True)\n",
    "    b3 = torch.cross(b1, b2)\n",
    "    return torch.stack([b1, b2, b3], dim=-1)\n",
    "\n",
    "def rotation_error(R_pred, R_gt):\n",
    "    \"\"\"Compute angular error in degrees between rotation matrices.\"\"\"\n",
    "    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = torch.diagonal(R_diff, dim1=1, dim2=2).sum(dim=1)\n",
    "    eps = 1e-6\n",
    "    angle_rad = torch.acos(torch.clamp((trace - 1) / 2, min=-1 + eps, max=1 - eps))\n",
    "    return torch.rad2deg(angle_rad)\n",
    "\n",
    "\n",
    "def geodesic_loss(R_pred, R_gt):\n",
    "    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = torch.diagonal(R_diff, dim1=1, dim2=2).sum(dim=1)\n",
    "    eps = 1e-6\n",
    "    angle = torch.acos(torch.clamp((trace - 1) / 2, -1 + eps, 1 - eps))\n",
    "    return angle.mean()\n",
    "\n",
    "def compute_rotation_matrix_from_ortho6d(poses_6d):\n",
    "    \"\"\"Convert 6D rotation representation to 3x3 rotation matrices.\"\"\"\n",
    "    x_raw = poses_6d[:, 0:3]\n",
    "    y_raw = poses_6d[:, 3:6]\n",
    "\n",
    "    x = F.normalize(x_raw, dim=1)\n",
    "    z = F.normalize(torch.cross(x, y_raw, dim=1), dim=1)\n",
    "    y = torch.cross(z, x, dim=1)\n",
    "\n",
    "    rot = torch.stack((x, y, z), dim=-1)  # Shape: [B, 3, 3]\n",
    "    return rot\n",
    "\n",
    "def combined_loss(output, target):\n",
    "    # Assuming output and target are dicts or tuples\n",
    "    # and you get translation and rotation like this:\n",
    "\n",
    "    pred_trans = output['trans']       # [B, 3]\n",
    "    gt_trans = target['trans']         # [B, 3]\n",
    "\n",
    "    pred_rot = output['rot']           # [B, 3, 3]\n",
    "    gt_rot = target['rot']             # [B, 3, 3]\n",
    "\n",
    "    # Translation loss (L2)\n",
    "    loss_trans = nn.functional.mse_loss(pred_trans, gt_trans)\n",
    "\n",
    "    # Rotation loss (geodesic)\n",
    "    loss_rot = geodesic_loss(pred_rot, gt_rot)\n",
    "\n",
    "    # Balance weights (tune as needed)\n",
    "    alpha, beta = 1.0, 1.0  # or try alpha=1.0, beta=10.0\n",
    "\n",
    "    return alpha * loss_trans + beta * loss_rot\n",
    "\n",
    "def rotation_error_deg(pred_6d, gt_6d):\n",
    "    R_pred = compute_rotation_matrix_from_ortho6d(pred_6d)\n",
    "    R_gt = compute_rotation_matrix_from_ortho6d(gt_6d)\n",
    "\n",
    "    R_diff = torch.bmm(R_pred.transpose(1, 2), R_gt)\n",
    "    trace = R_diff[:, 0, 0] + R_diff[:, 1, 1] + R_diff[:, 2, 2]\n",
    "    cos_theta = (trace - 1) / 2\n",
    "    cos_theta = torch.clamp(cos_theta, -1.0 + 1e-6, 1.0 - 1e-6)\n",
    "    theta = torch.acos(cos_theta)\n",
    "    return torch.rad2deg(theta).mean()\n",
    "\n",
    "def compute_errors(outputs, labels):\n",
    "    pred_trans = outputs[:, :3]\n",
    "    gt_trans = labels[:, :3]\n",
    "\n",
    "    pred_rot_6d = outputs[:, 3:9]\n",
    "    gt_rot_6d = labels[:, 3:9]\n",
    "\n",
    "    trans_rmse = torch.sqrt(F.mse_loss(pred_trans, gt_trans))\n",
    "    rot_rmse = rotation_error_deg(pred_rot_6d, gt_rot_6d)\n",
    "    return trans_rmse.item(), rot_rmse.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b2be1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_dataloader(split):\n",
    "    image_dir = os.path.join(DATASET_DIR, split, 'images')\n",
    "    label_csv = os.path.join(DATASET_DIR, split, 'labels.csv')\n",
    "    dataset = PoseDataset(image_dir, label_csv, transform)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=(split == 'train'))\n",
    "\n",
    "train_loader = get_dataloader('train')\n",
    "val_loader = get_dataloader('val')\n",
    "test_loader = get_dataloader('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce8a90",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_dataset_stats(loader):\n",
    "    all_labels = []\n",
    "    for _, labels in loader:\n",
    "        all_labels.append(labels)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Translation stats (x,y,z)\n",
    "    trans_stats = {\n",
    "        'min': all_labels[:, :3].min(dim=0)[0],\n",
    "        'max': all_labels[:, :3].max(dim=0)[0],\n",
    "        'mean': all_labels[:, :3].mean(dim=0),\n",
    "        'std': all_labels[:, :3].std(dim=0)\n",
    "    }\n",
    "\n",
    "    # Rotation stats (Angular)\n",
    "    rot_stats = {\n",
    "        'min': all_labels[:, 3:].min(dim=0)[0],\n",
    "        'max': all_labels[:, 3:].max(dim=0)[0],\n",
    "        'mean': all_labels[:, 3:].mean(dim=0),\n",
    "        'std': all_labels[:, 3:].std(dim=0),\n",
    "        'euler_min': None,  # Placeholder for Euler conversion min\n",
    "        'euler_max': None   # Placeholder for Euler conversion max\n",
    "    }\n",
    "\n",
    "    return trans_stats, rot_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stats for each dataset split\n",
    "train_trans_stats, train_rot_stats = get_dataset_stats(train_loader)\n",
    "val_trans_stats, val_rot_stats = get_dataset_stats(val_loader)\n",
    "test_trans_stats, test_rot_stats = get_dataset_stats(test_loader)\n",
    "\n",
    "print(f\"Train Translation stat: {train_trans_stats}      |       Train Rotation stat: {train_rot_stats}\")\n",
    "print(f\"Validation Translation stat: {val_trans_stats}     |       Validation Rotation stat: {val_rot_stats}\")\n",
    "print(f\"Test Translation stat: {test_trans_stats}      |       Test Rotation stat: {test_rot_stats}\")\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756fd4be",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ViT6DP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load pretrained ViT\n",
    "        self.backbone = timm.create_model('vit_base_patch16_224', \n",
    "                                       pretrained=True,\n",
    "                                       num_classes=0)  # Important: no default head\n",
    "        \n",
    "        # Custom head for 6D pose (3 translation + 6 rotation)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.backbone.num_features, 512),  # Use num_features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 9)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get features from backbone\n",
    "        features = self.backbone(x)  # Shape: [batch_size, num_features]\n",
    "        \n",
    "        # Pass through custom head\n",
    "        return self.head(features)  # Shape: [batch_size, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7dabf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "model = ViT6DP().to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8b63d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(validate=True):\n",
    "    now = [time.time()]\n",
    "    epochs_no_improve = 0\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"\\n\")\n",
    "        print(f\"EPOCH : {epoch+1}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            # AMP forward pass\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                outputs = model(images)\n",
    "                loss = combined_loss(outputs, labels)\n",
    "            \n",
    "            # AMP backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        now.append(time.time())\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Time per epoch {epoch + 1}: {int(now[epoch + 1] - now[epoch])}s\")\n",
    "\n",
    "        if validate:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            total_trans_rmse, total_rot_rmse = 0.0, 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images = images.to(DEVICE)\n",
    "                    labels = labels.to(DEVICE)\n",
    "                    outputs = model(images)\n",
    "\n",
    "                    loss = combined_loss(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    trans_rmse, rot_rmse = compute_errors(outputs, labels)\n",
    "                    total_trans_rmse += trans_rmse\n",
    "                    total_rot_rmse += rot_rmse\n",
    "\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "                print(f\"RMSE - Translation: {total_trans_rmse / len(val_loader):.4f}, \"\n",
    "                      f\"Rotation: {total_rot_rmse / len(val_loader):.4f}\")\n",
    "\n",
    "                # Scheduler step\n",
    "                scheduler.step(avg_val_loss)\n",
    "\n",
    "                if epoch != 0 and avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    epochs_no_improve = 0\n",
    "\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve == PATIENCE:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "        else:\n",
    "            print(\"\\nSkipping validation for this epoch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa981c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "# Increment version\n",
    "mod_id += 1\n",
    "gv['mod_id'] = mod_id\n",
    "with open(\"GlobVar.json\", \"w\") as file:\n",
    "    json.dump(gv, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7bbb06",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test_model(model, loader, mode='test'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_trans_rmse, total_rot_rmse = 0.0, 0.0\n",
    "    preds, gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            # Optimized device transfer\n",
    "            images = images.to(DEVICE, non_blocking=True)\n",
    "            labels = labels.to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            # FP32 inference for stable metrics\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = combined_loss(outputs, labels)\n",
    "            trans_rmse, rot_rmse = compute_errors(outputs, labels)\n",
    "            \n",
    "            # Accumulate\n",
    "            total_loss += loss.item()\n",
    "            total_trans_rmse += trans_rmse\n",
    "            total_rot_rmse += rot_rmse\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            gts.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"{mode.capitalize()} Loss: {avg_loss:.4f}\")\n",
    "    print(f\"RMSE - Translation: {total_trans_rmse/len(loader):.4f}, Rotation: {total_rot_rmse/len(loader):.4f}\")\n",
    "    \n",
    "    return preds, gts, avg_loss, total_trans_rmse, total_rot_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1102a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load model\n",
    "model = ViT6DP().to(DEVICE)  # Note: Different variable name than the function\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "\n",
    "# 2. Run evaluation using the TEST FUNCTION (not the model instance)\n",
    "predictions, ground_truths, test_avg_loss, test_total_trans_rmse, test_total_rot_rmse = test_model(\n",
    "    model=model,  # Pass the loaded model\n",
    "    loader=test_loader,\n",
    "    mode='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4da1f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to validate the model\n",
    "def validate_model(model_path):\n",
    "    model = ViT6DP().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    val_total_loss = 0.0\n",
    "    val_total_trans_rmse, val_total_rot_rmse = 0.0, 0.0\n",
    "    preds, gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = combined_loss(outputs, labels)\n",
    "            val_total_loss += loss.item()\n",
    "\n",
    "            trans_rmse, rot_rmse = compute_errors(outputs, labels)\n",
    "            val_total_trans_rmse += trans_rmse\n",
    "            val_total_rot_rmse += rot_rmse\n",
    "\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            gts.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_avg_loss = val_total_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {val_avg_loss:.4f}\")\n",
    "    print(f\"Validation RMSE - Translation: {val_total_trans_rmse / len(val_loader):.4f}, \"\n",
    "          f\"Rotation: {val_total_rot_rmse / len(val_loader):.4f}\")\n",
    "\n",
    "    return preds, gts, val_avg_loss, val_total_trans_rmse, val_total_rot_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f751e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions, val_ground_truths, val_avg_loss, val_total_trans_rmse, val_total_rot_rmse = validate_model(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d6f82",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893df1d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to calculate translation and rotation accuracy\n",
    "def calculate_translation_rmse(preds, gts):\n",
    "    \"\"\"Euclidean distance between predicted and GT translations (in meters).\"\"\"\n",
    "    errors = np.linalg.norm(preds - gts, axis=1)  # Shape: [N]\n",
    "    rmse = np.sqrt(np.mean(errors**2))\n",
    "    return rmse * 1000  # Convert to mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8e825",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "val_trans_accuracy, val_rot_accuracy = val_total_trans_rmse / len(val_loader), val_total_rot_rmse / len(val_loader)\n",
    "test_trans_accuracy, test_rot_accuracy = test_total_trans_rmse / len(test_loader), test_total_rot_rmse / len(test_loader)\n",
    "\n",
    "# Write evaluation report to markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d64842",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_content = f\"\"\"# Evaluation Results - Batch {BATCH_ID} - Model {mod_id-1}\n",
    "\n",
    "## Training Configuration\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Epochs: {NUM_EPOCHS}\n",
    "- Learning Rate: {LEARNING_RATE}\n",
    "- Translation Weight : {TRANS_WEIGHT}\n",
    "- Rotation Weight : {ROTATION_WEIGHT}\n",
    "- Angular Weight : {ANGULAR_WEIGHT}\n",
    "- Patience : {PATIENCE}\n",
    "- Image Size: {IMG_SIZE}\n",
    "- Device: {DEVICE}\n",
    "- Optimizer : Adam\n",
    "\n",
    "## Model Architecture\n",
    "- Backbone: ViT Base Patch16 224\n",
    "- Head: Linear(768->512->9)\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### Validation Set\n",
    "- Average Loss: {val_avg_loss:.4f}\n",
    "- Translation RMSE: {val_total_trans_rmse / len(val_loader):.4f}\n",
    "- Translation Accuracy: {val_trans_accuracy:.2f} cm\n",
    "- Rotation RMSE: {val_total_rot_rmse / len(val_loader):.4f}\n",
    "- Rotation Accuracy: {val_rot_accuracy:.2f}°\n",
    "\n",
    "### Test Set\n",
    "- Average Loss: {test_avg_loss:.4f}\n",
    "- Translation RMSE: {test_total_trans_rmse / len(test_loader):.4f}\n",
    "- Translation Accuracy: {test_trans_accuracy:.2f} cm\n",
    "- Rotation RMSE: {test_total_rot_rmse / len(test_loader):.4f}\n",
    "- Rotation Accuracy: {test_rot_accuracy:.2f}°\n",
    "\n",
    "## Dataset Statistics\n",
    "### Training Set\n",
    "- Translation range: [{train_trans_stats['min'].mean():.2f}, {train_trans_stats['max'].mean():.2f}] m\n",
    "\n",
    "### Validation Set\n",
    "- Translation range: [{val_trans_stats['min'].mean():.2f}, {val_trans_stats['max'].mean():.2f}] m\n",
    "\n",
    "### Test Set\n",
    "- Translation range: [{test_trans_stats['min'].mean():.2f}, {test_trans_stats['max'].mean():.2f}] m\n",
    "\n",
    "## File Locations\n",
    "- Dataset Directory: {DATASET_DIR}\n",
    "- Model Save Path: {MODEL_SAVE_PATH}\n",
    "\"\"\"\n",
    "\n",
    "eval_path = os.path.join(BASE_DIR, f\"model/ViT6DP_EVAL_batch{BATCH_ID}.{mod_id-1}.md\")\n",
    "with open(eval_path, 'w') as f:\n",
    "    f.write(eval_content)\n",
    "\n",
    "print(f\"Evaluation report saved to: {eval_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to CSV\n",
    "csv_path = os.path.join(BASE_DIR, \"model/eval_results.csv\")\n",
    "write_header = not os.path.exists(csv_path)\n",
    "\n",
    "csv_data = {\n",
    "    'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_id': BATCH_ID,\n",
    "    'model_id': mod_id-1,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'translation_weight':TRANS_WEIGHT,\n",
    "    'rotation_weight' : ROTATION_WEIGHT,\n",
    "    'angular_weight' : ANGULAR_WEIGHT,\n",
    "    'patience' : PATIENCE,\n",
    "    'test_loss': test_avg_loss,\n",
    "    'test_translation_rmse': test_total_trans_rmse / len(test_loader),\n",
    "    'test_rotation_rmse': test_total_rot_rmse / len(test_loader),\n",
    "    'validation_loss': val_avg_loss,\n",
    "    'validation_translation_rmse': val_total_trans_rmse / len(val_loader),\n",
    "    'validation_rotation_rmse': val_total_rot_rmse / len(val_loader),\n",
    "    'model_path': MODEL_SAVE_PATH,\n",
    "    'eval_path' : eval_path\n",
    "}\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_path, 'a', newline='') as csvfile:\n",
    "    fieldnames = csv_data.keys()\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(csv_data)\n",
    "\n",
    "print(f\"Results appended to CSV: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
