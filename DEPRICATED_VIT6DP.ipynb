{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIT6D.ipynb refactored for Minty (Linux Mint)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d4ae2",
   "metadata": {},
   "source": [
    "PREREQUISITES AND DECLARATIONS AND YADA YADA YADA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5b772",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "with open(\"GlobVar.json\", \"r\") as file:\n",
    "    gv = json.load(file)\n",
    "mod_id = gv['mod_id']\n",
    "# Constants and environment setup\n",
    "BATCH_ID = 3\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "IMG_SIZE = 224  # Required input size for ViT\n",
    "\n",
    "# Paths assume Linux-style forward slashes\n",
    "BASE_DIR = os.path.expanduser(\"~/SKRIPSI/SCRIPTS\")  # Refactor path to be absolute\n",
    "DATASET_DIR = os.path.join(BASE_DIR, f\"dataset/batch{BATCH_ID}\")\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_DIR, f\"model/ViT6DP_batch{BATCH_ID}.{mod_id}.pth\")\n",
    "\n",
    "# Use CUDA if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f4d3a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_csv, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = pd.read_csv(label_csv)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['image_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = torch.tensor(row[1:].values.astype('float32'))  # x, y, z, pitch, roll, yaw\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10edf29",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a43fbf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_dataloader(split):\n",
    "    image_dir = os.path.join(DATASET_DIR, split, 'images')\n",
    "    label_csv = os.path.join(DATASET_DIR, split, 'labels.csv')\n",
    "    dataset = PoseDataset(image_dir, label_csv, transform)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=(split == 'train'))\n",
    "\n",
    "train_loader = get_dataloader('train')\n",
    "val_loader = get_dataloader('val')\n",
    "test_loader = get_dataloader('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_stats(loader):\n",
    "    all_labels = []\n",
    "    for _, labels in loader:\n",
    "        all_labels.append(labels)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    # Translation stats (x,y,z)\n",
    "    trans_stats = {\n",
    "        'min': all_labels[:, :3].min(dim=0)[0],\n",
    "        'max': all_labels[:, :3].max(dim=0)[0],\n",
    "        'mean': all_labels[:, :3].mean(dim=0),\n",
    "        'std': all_labels[:, :3].std(dim=0)\n",
    "    }\n",
    "    \n",
    "    # Rotation stats (pitch, roll, yaw)\n",
    "    rot_stats = {\n",
    "        'min': all_labels[:, 3:].min(dim=0)[0],\n",
    "        'max': all_labels[:, 3:].max(dim=0)[0],\n",
    "        'mean': all_labels[:, 3:].mean(dim=0),\n",
    "        'std': all_labels[:, 3:].std(dim=0)\n",
    "    }\n",
    "    \n",
    "    return trans_stats, rot_stats\n",
    "\n",
    "# Get stats for each dataset split\n",
    "train_trans_stats, train_rot_stats = get_dataset_stats(train_loader)\n",
    "val_trans_stats, val_rot_stats = get_dataset_stats(val_loader)\n",
    "test_trans_stats, test_rot_stats = get_dataset_stats(test_loader)\n",
    "print(f\"Train Translation stat:{train_trans_stats}      |       Train Rotation stat: {train_rot_stats}\")\n",
    "print(f\"Validation Translation stat:{val_trans_stats}     |       Validation Rotation stat: {val_rot_stats}\")\n",
    "print(f\"Test Translation stat:{test_trans_stats}      |       Test Rotation stat: {test_rot_stats}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e2da8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ViT6DP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        self.backbone.head = nn.Sequential(\n",
    "            nn.Linear(self.backbone.head.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 9)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af30195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(pred, target):\n",
    "    pred_trans, pred_rot = pred[:, :3], pred[:, 3:]\n",
    "    target_trans, target_rot = target[:, :3], target[:, 3:]\n",
    "\n",
    "    trans_rmse = torch.sqrt(nn.MSELoss()(pred_trans, target_trans))\n",
    "    rot_rmse = torch.sqrt(nn.MSELoss()(pred_rot, target_rot))\n",
    "\n",
    "    return trans_rmse.item(), rot_rmse.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_loss(pred, target, alpha=1.0, beta=1.0):\n",
    "    pred_trans, pred_rot = pred[:, :3], pred[:, 3:]\n",
    "    target_trans, target_rot = target[:, :3], target[:, 3:]\n",
    "\n",
    "    trans_loss = nn.MSELoss()(pred_trans, target_trans)\n",
    "    rot_loss = nn.MSELoss()(pred_rot, target_rot)\n",
    "\n",
    "    return alpha * trans_loss + beta * rot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021f0ea",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "model = ViT6DP().to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818da2e",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d6f41",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(validate=True):\n",
    "    now = [time.time()]\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"\\n\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)  # shape: [B, 9] -> [tx, ty, tz, r1...r6]\n",
    "\n",
    "            outputs = model(images)  # no normalize_pose anymore\n",
    "\n",
    "            loss = combined_loss(outputs, labels, alpha=1.0, beta=1.0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        now.append(time.time())\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Time per epoch {epoch + 1}: {int(now[epoch + 1] - now[epoch])}s\")\n",
    "\n",
    "        if validate:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            total_trans_rmse, total_rot_rmse = 0.0, 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images = images.to(DEVICE)\n",
    "                    labels = labels.to(DEVICE)\n",
    "                    outputs = model(images)\n",
    "\n",
    "                    loss = combined_loss(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    trans_rmse, rot_rmse = compute_rmse(outputs, labels)\n",
    "                    total_trans_rmse += trans_rmse\n",
    "                    total_rot_rmse += rot_rmse\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"RMSE - Translation: {total_trans_rmse / len(val_loader):.4f}, \"\n",
    "                  f\"Rotation: {total_rot_rmse / len(val_loader):.4f}\")\n",
    "        else:\n",
    "            print(\"Skipping validation for this epoch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bafc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1a361",
   "metadata": {},
   "source": [
    "SAVE + INCREMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "# Increment\n",
    "mod_id += 1\n",
    "gv['mod_id'] = mod_id\n",
    "# Save the updated JSON back to the file\n",
    "with open(\"GlobVar.json\", \"w\") as file:\n",
    "    json.dump(gv, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a7917",
   "metadata": {},
   "source": [
    "TEST THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac53c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_model(model_path):\n",
    "    model = ViT6DP().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    test_total_loss = 0.0\n",
    "    test_total_trans_rmse, test_total_rot_rmse = 0.0, 0.0\n",
    "    preds, gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = combined_loss(outputs, labels)\n",
    "            test_total_loss += loss.item()\n",
    "\n",
    "            trans_rmse, rot_rmse = compute_rmse(outputs, labels)\n",
    "            test_total_trans_rmse += trans_rmse\n",
    "            test_total_rot_rmse += rot_rmse\n",
    "\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            gts.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_avg_loss = test_total_loss / len(test_loader)\n",
    "    print(f\"Test Loss: {test_avg_loss:.4f}\")\n",
    "    print(f\"Test RMSE - Translation: {test_total_trans_rmse / len(test_loader):.4f}, \"\n",
    "          f\"Rotation: {test_total_rot_rmse / len(test_loader):.4f}\")\n",
    "\n",
    "    return preds, gts, test_avg_loss, test_total_trans_rmse, test_total_rot_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d947127",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "predictions, ground_truths, test_avg_loss, test_total_trans_rmse, test_total_rot_rmse = test_model(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e6f140",
   "metadata": {},
   "source": [
    "VALIDATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1ff78",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def validate_model(model_path):\n",
    "    model = ViT6DP().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    val_total_loss = 0.0\n",
    "    val_total_trans_rmse, val_total_rot_rmse = 0.0, 0.0\n",
    "    preds, gts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = combined_loss(outputs, labels)\n",
    "            val_total_loss += loss.item()\n",
    "\n",
    "            trans_rmse, rot_rmse = compute_rmse(outputs, labels)\n",
    "            val_total_trans_rmse += trans_rmse\n",
    "            val_total_rot_rmse += rot_rmse\n",
    "\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            gts.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_avg_loss = val_total_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {val_avg_loss:.4f}\")\n",
    "    print(f\"Validation RMSE - Translation: {val_total_trans_rmse / len(val_loader):.4f}, \"\n",
    "          f\"Rotation: {val_total_rot_rmse / len(val_loader):.4f}\")\n",
    "\n",
    "    return preds, gts, val_avg_loss, val_total_trans_rmse, val_total_rot_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db70a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions, val_ground_truths, val_avg_loss, val_total_trans_rmse, val_total_rot_rmse = validate_model(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6003a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c71dd9",
   "metadata": {},
   "source": [
    "CALCULATIONS AND SUCH (ALSO OUTPUTTING TO MD AND CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ec21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_to_rotation_matrix(r1, r2):\n",
    "    \"\"\"Convert orthogonalized r1 and r2 vectors into a rotation matrix.\"\"\"\n",
    "    r1 = r1 / np.linalg.norm(r1)\n",
    "    r2 = r2 / np.linalg.norm(r2)\n",
    "    r3 = np.cross(r1, r2)\n",
    "    return np.stack([r1, r2, r3], axis=1)\n",
    "\n",
    "def calculate_translation_rmse(preds, gts):\n",
    "    \"\"\"Euclidean distance between predicted and GT translations (in meters).\"\"\"\n",
    "    errors = np.linalg.norm(preds - gts, axis=1)  # Shape: [N]\n",
    "    rmse = np.sqrt(np.mean(errors**2))\n",
    "    return rmse * 1000  # Convert to mm\n",
    "\n",
    "def calculate_rotation_rmse(preds_r1r2, gts_r1r2):\n",
    "    \"\"\"Angular difference (in degrees) between predicted and GT rotation matrices.\"\"\"\n",
    "    angles = []\n",
    "    for pred, gt in zip(preds_r1r2, gts_r1r2):\n",
    "        R_pred = vectors_to_rotation_matrix(pred[:3], pred[3:])\n",
    "        R_gt = vectors_to_rotation_matrix(gt[:3], gt[3:])\n",
    "        R_diff = R_pred.T @ R_gt\n",
    "        angle = np.arccos(np.clip((np.trace(R_diff) - 1) / 2.0, -1.0, 1.0))\n",
    "        angles.append(np.degrees(angle))\n",
    "    return np.sqrt(np.mean(np.array(angles)**2))  # RMSE in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trans_accuracy, val_rot_accuracy = val_total_trans_rmse / len(val_loader),  val_total_rot_rmse / len(val_loader)\n",
    "test_trans_accuracy, test_rot_accuracy = test_total_trans_rmse / len(test_loader),  test_total_rot_rmse / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rot_mag_min = train_rot_stats['min'].norm().item()\n",
    "train_rot_mag_max = train_rot_stats['max'].norm().item()\n",
    "val_rot_mag_min = val_rot_stats['min'].norm().mean().item()\n",
    "val_rot_mag_max = val_rot_stats['max'].norm().mean().item()\n",
    "test_rot_mag_min = test_rot_stats['min'].norm().mean().item()\n",
    "test_rot_mag_max = test_rot_stats['max'].norm().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61826ebd",
   "metadata": {},
   "source": [
    "WRITE TO MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_content = f\"\"\"# Evaluation Results - Batch {BATCH_ID}\n",
    "\n",
    "## Training Configuration\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Epochs: {NUM_EPOCHS}\n",
    "- Learning Rate: {LEARNING_RATE}\n",
    "- Image Size: {IMG_SIZE}\n",
    "- Device: {DEVICE}\n",
    "- Optimizer : Adam\n",
    "\n",
    "## Model Architecture\n",
    "- Backbone: ViT Base Patch16 224\n",
    "- Head: Linear(768->512->6)\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### Validation Set\n",
    "- Average Loss: {val_avg_loss:.4f}\n",
    "- Translation RMSE: {val_total_trans_rmse / len(val_loader):.4f}\n",
    "- Translation Accuracy: {val_trans_accuracy:.2f} cm\n",
    "- Rotation RMSE: {val_total_rot_rmse / len(val_loader):.4f}\n",
    "- Rotation Accuracy: {val_rot_accuracy:.2f}°\n",
    "\n",
    "### Test Set\n",
    "- Average Loss: {test_avg_loss:.4f}\n",
    "- Translation RMSE: {test_total_trans_rmse / len(test_loader):.4f}\n",
    "- Translation Accuracy: {test_trans_accuracy:.2f} cm\n",
    "- Rotation RMSE: {test_total_rot_rmse / len(test_loader):.4f}\n",
    "- Rotation Accuracy: {test_rot_accuracy:.2f}°\n",
    "\n",
    "## Dataset Statistics\n",
    "### Training Set\n",
    "- Translation range: [{train_trans_stats['min'].mean():.2f}, {train_trans_stats['max'].mean():.2f}] m\n",
    "- Rotation magnitude range: [{train_rot_mag_min:.2f}, {train_rot_mag_max:.2f}]\n",
    "\n",
    "### Validation Set\n",
    "- Translation range: [{val_trans_stats['min'].mean():.2f}, {val_trans_stats['max'].mean():.2f}] m\n",
    "- Rotation magnitude range: [{val_rot_mag_min:.2f}, {val_rot_mag_max:.2f}]\n",
    "\n",
    "### Test Set\n",
    "- Translation range: [{test_trans_stats['min'].mean():.2f}, {test_trans_stats['max'].mean():.2f}] m\n",
    "- Rotation magnitude range: [{test_rot_mag_min:.2f}, {test_rot_mag_max:.2f}]\n",
    "\n",
    "## File Locations\n",
    "- Dataset Directory: {DATASET_DIR}\n",
    "- Model Save Path: {MODEL_SAVE_PATH}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "eval_path = os.path.join(BASE_DIR, f\"model/ViT6DP_EVAL_batch{BATCH_ID}.{mod_id-1}.md\")\n",
    "with open(eval_path, 'w') as f:\n",
    "    f.write(eval_content)\n",
    "    \n",
    "print(f\"Evaluation report saved to: {eval_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f43d3d",
   "metadata": {},
   "source": [
    "WRITE TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e89dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the CSV file path\n",
    "csv_path = os.path.join(BASE_DIR, \"model/eval_results.csv\")\n",
    "\n",
    "# Check if CSV exists to determine if we need to write headers\n",
    "write_header = not os.path.exists(csv_path)\n",
    "\n",
    "\n",
    "csv_data = {\n",
    "    'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_id': BATCH_ID,\n",
    "    'model_id': mod_id-1,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': NUM_EPOCHS,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'test_loss': test_avg_loss,\n",
    "    'test_translation_rmse': test_total_trans_rmse / len(test_loader),\n",
    "    'test_rotation_rmse': test_total_rot_rmse / len(test_loader),\n",
    "    'validation_loss': val_avg_loss,\n",
    "    'validation_translation_rmse': val_total_trans_rmse / len(val_loader),\n",
    "    'validation_rotation_rmse': val_total_rot_rmse / len(val_loader),\n",
    "    'model_path': MODEL_SAVE_PATH,\n",
    "    'eval_path' : eval_path\n",
    "}\n",
    "\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_path, 'a', newline='') as csvfile:\n",
    "    fieldnames = csv_data.keys()\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(csv_data)\n",
    "\n",
    "print(f\"Results appended to CSV: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
